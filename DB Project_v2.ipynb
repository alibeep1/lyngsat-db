{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f04de3",
   "metadata": {},
   "source": [
    "### Perform Necessary Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b3aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9bb4e",
   "metadata": {},
   "source": [
    "### Define Helper Functions\n",
    "We will use those throughout our notebook as they make our code resuable, extensible, and readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e8c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comprehension(matrix):\n",
    "     return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "def extract_rows_from_tables(tables):\n",
    "    rows = []\n",
    "# Now, iterate over each table in the ResultSet\n",
    "    for table in tables:\n",
    "        # For each table, you can now call find_all on it\n",
    "        rows.append(table.find_all('tr'))\n",
    "        # Do something with rows, e.g., print them\n",
    "        \n",
    "    \n",
    "    return rows\n",
    "\n",
    "def extract_raw_data(column_data,recurse_thru_a = False, find_tags = 'td', extract_provider_links = False, pattern =''):\n",
    "    raw_data = []\n",
    "    for row in column_data:\n",
    "        row_data = row.find_all(find_tags)\n",
    "        individual_row_data = []\n",
    "        \n",
    "        if(recurse_thru_a == True):\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "#             print(\"extracted links: \", extracted_links)\n",
    "            link_to_page = extracted_links[0] if extracted_links else None\n",
    "#             print(f'Link to page: {link_to_page}')\n",
    "            individual_row_data.append(link_to_page)\n",
    "        # print(\"row_data[0] = \", row_data[0])\n",
    "        individual_row_data +=[data.text.strip() for data in row_data]\n",
    "        \n",
    "        if extract_provider_links == True:\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "            for element in extracted_links:\n",
    "                matches = re.findall(pattern, element)\n",
    "                if(matches):\n",
    "                    individual_row_data.append(element)\n",
    "\n",
    "        raw_data.append(individual_row_data)\n",
    "    return raw_data\n",
    "def extract_providers_link(column_data, find_tags = 'td',  pattern = ''):\n",
    "    raw_data = []\n",
    "    for row in column_data:\n",
    "        row_data = row.find_all(find_tags)\n",
    "        individual_row_data = []\n",
    "        \n",
    "       \n",
    "        extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "        \n",
    "        \n",
    "#         print(extracted_links)\n",
    "        for element in extracted_links:\n",
    "            matches = re.findall(pattern, element)\n",
    "            if(matches):\n",
    "                # print(f\"'{element}' contains {len(matches)} occurrence(s) of {pattern}\")\n",
    "                individual_row_data +=[data.text.strip() for data in row_data]\n",
    "                individual_row_data.append(element)\n",
    "                raw_data.append(individual_row_data)\n",
    "\n",
    "                \n",
    "#         individual_row_data +=[data.text.strip() for data in row_data]\n",
    "#         print(f'{individual_row_data}')\n",
    "    return raw_data\n",
    "def append_data_to_df(df, raw_data):\n",
    "    for data in raw_data:\n",
    "#         print(f'data: {data}')\n",
    "        length = len(df)\n",
    "        df.loc[length] = data\n",
    "    return df\n",
    "\n",
    "def preprocess_extracted_sats(cont_sats_extracted, row_names, attr_index = 0):\n",
    "    for index, sat in enumerate(cont_sats_extracted):\n",
    "#         print(\"len(sat) = > \",len(sat))\n",
    "#         print(\"len(row_names) = > \",len(sat))\n",
    "        \n",
    "        if len(sat) <= len(row_names) - 1:\n",
    "#             print(f'found a channel \"{sat}\"')\n",
    "            sat_dir = cont_sats_extracted[index - 1][attr_index]\n",
    "#             print(f'New Attr \"{sat_dir}\"')\n",
    "            sat.insert(attr_index, sat_dir)\n",
    "            cont_sats_extracted[index] = sat\n",
    "#             print(f'New channel \"{cont_sats_extracted[index]}\"')\n",
    "\n",
    "    return cont_sats_extracted\n",
    "\n",
    "def append_region_to_extracted_sats(cont_sats_extracted, continent):\n",
    "    for index, sat in enumerate(cont_sats_extracted):\n",
    "        cont_sats_extracted[index] =  sat + [continent]\n",
    "    return cont_sats_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6f361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_data_alt(column_data,recurse_thru_a = False, find_tags = 'td', extract_provider_links = False, pattern =''):\n",
    "    raw_data = []\n",
    "    for row in column_data:\n",
    "        row_data = row.find_all(find_tags)\n",
    "        columns = row_data\n",
    "        individual_row_data = []\n",
    "        \n",
    "        \n",
    "        if(recurse_thru_a == True):\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "            link_to_page = extracted_links[0] if extracted_links else None\n",
    "            individual_row_data.append(link_to_page)\n",
    "\n",
    "        individual_row_data +=[data.text.strip() for data in row_data]\n",
    "        \n",
    "       \n",
    "        # print(len(columns))\n",
    "        if len(columns) == 10:\n",
    "            font = columns[0].find_all('font')[0]\n",
    "            # print('font: ', font)\n",
    "            \n",
    "            br = font.find_all('br')\n",
    "            # print('br: ', br)\n",
    "            freq = None\n",
    "            beam = None\n",
    "            eirp = None\n",
    "            for i, thing in enumerate(br):\n",
    "                if i == 0:\n",
    "                    freq = thing.previous_sibling.get_text(strip=True)\n",
    "                elif i == 1:\n",
    "                    beam = thing.previous_sibling.get_text(strip=True)\n",
    "                else:\n",
    "                    eirp = thing.next_sibling.get_text(strip=True) if thing.next_sibling else None\n",
    "            # print(f'freq: {freq}, beam: {beam}, eirp: {eirp}')\n",
    "        individual_row_data.append(freq)\n",
    "        individual_row_data.append(beam)\n",
    "        individual_row_data.append(eirp)\n",
    "        \n",
    "        if len(columns) == 10:\n",
    "            fonts = columns[8].find_all('font')\n",
    "            # print('fonts for network: ', fonts)\n",
    "            fonts = [bs(str(data).replace('<br/>',',')).text for data in fonts]\n",
    "            first_font = fonts[0] if fonts else None\n",
    "            individual_row_data.append(first_font)\n",
    "        else:\n",
    "            fonts = columns[6].find_all('font')\n",
    "            fonts = [bs(str(data).replace('<br/>',',')).text for data in fonts]\n",
    "            first_font = fonts[0] if fonts else None\n",
    "            individual_row_data.append(first_font)\n",
    "            # individual_row_data += fonts.pop()\n",
    "        # print('fonts: ', fonts)\n",
    "                        \n",
    "        # individual_row_data +=[data.text.strip() for data in row_data]\n",
    "        \n",
    "        # individual_row_data +=[BeautifulSoup(str(data).replace('<br/>',',')).text for data in row_data]\n",
    "        \n",
    "        if extract_provider_links == True:\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "            for element in extracted_links:\n",
    "                matches = re.findall(pattern, element)\n",
    "                if(matches):\n",
    "                    individual_row_data.append(element)\n",
    "\n",
    "        raw_data.append(individual_row_data)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05acdb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_char_count(df_column):\n",
    "    lengths = df_column.str.len()\n",
    "    # Find the index of the longest string\n",
    "    index_of_max = lengths.idxmax()\n",
    "    print(index_of_max, ', ',df_column.iloc[index_of_max])\n",
    "    return df_column.str.len().max()\n",
    "\n",
    "def display_max_length(df):\n",
    "    for i, col in enumerate(df.keys()):\n",
    "        char_count = max_char_count(df[col].astype(str))\n",
    "        print(f'COLUMN: {col},  MAX_LENGTH: {char_count}')\n",
    "\n",
    "\n",
    "def match_first_part(string):\n",
    "    match = re.search(r'^[^(]+', string)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return string\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2e1d9",
   "metadata": {},
   "source": [
    "## Extract Satellites for All Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc79721",
   "metadata": {},
   "source": [
    "### List Continents and concatenate with their url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "b315fc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.lyngsat.com/asia.html',\n",
       " 'https://www.lyngsat.com/europe.html',\n",
       " 'https://www.lyngsat.com/atlantic.html',\n",
       " 'https://www.lyngsat.com/america.html']"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continents = ['asia', 'europe', 'atlantic', 'america']\n",
    "baseurl = 'https://www.lyngsat.com/'\n",
    "\n",
    "# Construct the URLs\n",
    "endpoints = []\n",
    "for c in continents:\n",
    "    endpoints.append(f'{baseurl}{c}.html')\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660763f",
   "metadata": {},
   "source": [
    "### Scrape each Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "4d934f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "satellites = []\n",
    "for index, c in enumerate(continents):\n",
    "    page = requests.get(endpoints[index])\n",
    "    soup = bs(page.text, 'html')\n",
    "    \n",
    "    \n",
    "    satellites.append({f'{c}':soup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5076b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_names = ['Position','Name','Frequency','Launch_Date']\n",
    "cont_sats_df = pd.DataFrame(columns = row_names)\n",
    "# \n",
    "for index, cont in enumerate(continents):\n",
    "#     print('continent: ',cont)\n",
    "    cont_sats_raw = satellites[index][cont]\n",
    "    table = cont_sats_raw.find_all('table')[11]\n",
    "    \n",
    "    column_data = table.find_all('tr')\n",
    "    \n",
    "    cont_sats_extracted = extract_raw_data(column_data)\n",
    "    print(cont_sats_extracted[1:])\n",
    "    # cont_sats_extracted = preprocess_extracted_sats(cont_sats_extracted, row_names)\n",
    "    # cont_sats_extracted = append_region_to_extracted_sats(cont_sats_extracted, cont)    \n",
    "    # print((cont_sats_extracted[0][1]))\n",
    "    # cont_sats_df.loc[len(cont_sats_df)] = \n",
    "    temp_df = pd.DataFrame(cont_sats_extracted[1:], columns=row_names)\n",
    "    cont_sats_df = pd.concat([cont_sats_df, temp_df], ignore_index=True)\n",
    "    # cont_sats_df = append_data_to_df(cont_sats_df, cont_sats_extracted[1:])\n",
    "    \n",
    "# Switch the first (position) with the second column (name)\n",
    "cont_sats_df = cont_sats_df.iloc[:, [1, 0] + list(range(2, len(cont_sats_df.columns)))]\n",
    "cont_sats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "65b74a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Launch_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSS 9</td>\n",
       "      <td>177.0°W</td>\n",
       "      <td>C</td>\n",
       "      <td>190104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intelsat 18</td>\n",
       "      <td>180.0°E</td>\n",
       "      <td>CKu</td>\n",
       "      <td>231124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eutelsat 172B</td>\n",
       "      <td>172.0°E</td>\n",
       "      <td>CKu</td>\n",
       "      <td>240306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Horizons 3e</td>\n",
       "      <td>169.0°E</td>\n",
       "      <td>Ku</td>\n",
       "      <td>240302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intelsat 19</td>\n",
       "      <td>166.0°E</td>\n",
       "      <td>CKu</td>\n",
       "      <td>240401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Galaxy 33</td>\n",
       "      <td>133.0°W</td>\n",
       "      <td>CKu</td>\n",
       "      <td>230926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>SES 22</td>\n",
       "      <td>135.0°W</td>\n",
       "      <td>C</td>\n",
       "      <td>240302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>SES 19</td>\n",
       "      <td>135.0°W</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>AMC 6</td>\n",
       "      <td>139.0°W</td>\n",
       "      <td>C</td>\n",
       "      <td>210906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Galaxy 13/Horizons 1</td>\n",
       "      <td>150.0°W</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name Position Frequency Launch_Date\n",
       "0                   NSS 9  177.0°W         C      190104\n",
       "1             Intelsat 18  180.0°E       CKu      231124\n",
       "2           Eutelsat 172B  172.0°E       CKu      240306\n",
       "3             Horizons 3e  169.0°E        Ku      240302\n",
       "4             Intelsat 19  166.0°E       CKu      240401\n",
       "..                    ...      ...       ...         ...\n",
       "223             Galaxy 33  133.0°W       CKu      230926\n",
       "224                SES 22  135.0°W         C      240302\n",
       "225                SES 19  135.0°W                      \n",
       "226                 AMC 6  139.0°W         C      210906\n",
       "227  Galaxy 13/Horizons 1  150.0°W                      \n",
       "\n",
       "[228 rows x 4 columns]"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_sats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d635d",
   "metadata": {},
   "source": [
    "### Get Launching Rockets Details\n",
    "\n",
    "Now, since we have the satellites, we just need to get their launching rockets & dates into a dataframe, and then merge those two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "637b6139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.lyngsat.com/tracker/asia.html',\n",
       " 'https://www.lyngsat.com/tracker/europe.html',\n",
       " 'https://www.lyngsat.com/tracker/atlantic.html',\n",
       " 'https://www.lyngsat.com/tracker/america.html']"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continents = ['asia', 'europe', 'atlantic', 'america']\n",
    "baseurl = 'https://www.lyngsat.com/tracker/'\n",
    "\n",
    "# Construct the URLs\n",
    "endpoints = []\n",
    "for c in continents:\n",
    "    endpoints.append(f'{baseurl}{c}.html')\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "d8400915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_rockets = []\n",
    "for index, c in enumerate(continents):\n",
    "    page = requests.get(endpoints[index])\n",
    "    soup = bs(page.text, 'html')    \n",
    "    \n",
    "    sat_rockets.append({f'{c}':soup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666de34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sat_rockets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cb2d3",
   "metadata": {},
   "source": [
    "#### Get the launching URL for each satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "acc96590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Position</th>\n",
       "      <th>URL</th>\n",
       "      <th>Name</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Launch Date</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/TDRS-8.html</td>\n",
       "      <td>TDRS 8 (incl. 11.4°)</td>\n",
       "      <td></td>\n",
       "      <td>240319</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/TDRS-10.html</td>\n",
       "      <td>TDRS 10 (incl. 9.4°)</td>\n",
       "      <td></td>\n",
       "      <td>240319</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/TDRS-11.html</td>\n",
       "      <td>TDRS 11 (incl. 2.7°)</td>\n",
       "      <td></td>\n",
       "      <td>240320</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>177.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/NSS-9.html</td>\n",
       "      <td>NSS 9</td>\n",
       "      <td>C</td>\n",
       "      <td>240319</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/Yamal-300K.html</td>\n",
       "      <td>Yamal 300K</td>\n",
       "      <td></td>\n",
       "      <td>240319</td>\n",
       "      <td>asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>135.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/SES-19.html</td>\n",
       "      <td>SES 19</td>\n",
       "      <td></td>\n",
       "      <td>240319</td>\n",
       "      <td>america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>138.9°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/Spaceway-2.html</td>\n",
       "      <td>Spaceway 2 (incl. 3.4°)</td>\n",
       "      <td></td>\n",
       "      <td>240319</td>\n",
       "      <td>america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>139.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/AMC-6.html</td>\n",
       "      <td>AMC 6</td>\n",
       "      <td>C</td>\n",
       "      <td>240319</td>\n",
       "      <td>america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>139.2°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/Eutelsat-139-W...</td>\n",
       "      <td>Eutelsat 139 West A (incl. 3.7°)</td>\n",
       "      <td></td>\n",
       "      <td>240319</td>\n",
       "      <td>america</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>150.0°W</td>\n",
       "      <td>https://www.lyngsat.com/tracker/Galaxy-13-Hori...</td>\n",
       "      <td>Galaxy 13/Horizons 1</td>\n",
       "      <td></td>\n",
       "      <td>240319</td>\n",
       "      <td>america</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Position                                                URL  \\\n",
       "0    167.0°W        https://www.lyngsat.com/tracker/TDRS-8.html   \n",
       "1    171.0°W       https://www.lyngsat.com/tracker/TDRS-10.html   \n",
       "2    174.0°W       https://www.lyngsat.com/tracker/TDRS-11.html   \n",
       "3    177.0°W         https://www.lyngsat.com/tracker/NSS-9.html   \n",
       "4    177.0°W    https://www.lyngsat.com/tracker/Yamal-300K.html   \n",
       "..       ...                                                ...   \n",
       "308  135.0°W        https://www.lyngsat.com/tracker/SES-19.html   \n",
       "309  138.9°W    https://www.lyngsat.com/tracker/Spaceway-2.html   \n",
       "310  139.0°W         https://www.lyngsat.com/tracker/AMC-6.html   \n",
       "311  139.2°W  https://www.lyngsat.com/tracker/Eutelsat-139-W...   \n",
       "312  150.0°W  https://www.lyngsat.com/tracker/Galaxy-13-Hori...   \n",
       "\n",
       "                                 Name Frequency Launch Date   Region  \n",
       "0                TDRS 8 (incl. 11.4°)                240319     asia  \n",
       "1                TDRS 10 (incl. 9.4°)                240319     asia  \n",
       "2                TDRS 11 (incl. 2.7°)                240320     asia  \n",
       "3                               NSS 9         C      240319     asia  \n",
       "4                          Yamal 300K                240319     asia  \n",
       "..                                ...       ...         ...      ...  \n",
       "308                            SES 19                240319  america  \n",
       "309           Spaceway 2 (incl. 3.4°)                240319  america  \n",
       "310                             AMC 6         C      240319  america  \n",
       "311  Eutelsat 139 West A (incl. 3.7°)                240319  america  \n",
       "312              Galaxy 13/Horizons 1                240319  america  \n",
       "\n",
       "[313 rows x 6 columns]"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "row_names = ['URL','Position','Name','Frequency', 'Launch Date', 'Region']\n",
    "cont_sats_df = pd.DataFrame(columns=row_names)\n",
    "\n",
    "# index = 0\n",
    "# cont = 'asia'\n",
    "for index, cont in enumerate(continents):\n",
    "\n",
    "    cont_sats_rockets = sat_rockets[index][cont]\n",
    "    # cont_sats_rockets = sat_rockets[2]['atlantic']\n",
    "\n",
    "    # print(cont_sats_rockets)\n",
    "\n",
    "    table = cont_sats_rockets.find_all('table')[11]\n",
    "    #     print(table)\n",
    "\n",
    "    column_data = table.find_all('tr')\n",
    "    #     print(column_data)\n",
    "\n",
    "    cont_sats_extracted = extract_raw_data(column_data, True)\n",
    "\n",
    "    # cont_sats_extracted = preprocess_extracted_sats(cont_sats_extracted, row_names)\n",
    "    cont_sats_extracted = append_region_to_extracted_sats(cont_sats_extracted, cont)    \n",
    "\n",
    "    # print(len(cont_sats_extracted))\n",
    "\n",
    "\n",
    "    cont_sats_df = append_data_to_df(cont_sats_df, cont_sats_extracted[1:])\n",
    "    # cont_sats_df =  cont_sats_extracted\n",
    "        \n",
    "    \n",
    "# Switch the first (position) with the second column (name)\n",
    "cont_sats_df = cont_sats_df.iloc[:, [1, 0] + list(range(2, len(cont_sats_df.columns)))]\n",
    "cont_sats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b83561",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_sats_df['URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3550387",
   "metadata": {},
   "source": [
    "#### Get The Data\n",
    "for each satellite URL obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b2bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rocket_list = []\n",
    "rocket_date = []\n",
    "for index, cont in enumerate(cont_sats_df['URL']):\n",
    "    url = cont_sats_df['URL'][index]\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html')\n",
    "#     soup.find_all('font')[13]\n",
    "\n",
    "    entry  = soup.find_all('font')\n",
    "    \n",
    "    entry = ' '.join(map(str,entry))\n",
    "    sat_name = cont_sats_df['Name'][index]\n",
    "\n",
    "    x = re.search(\"launched with (.+) \\d\\d\\d\\d\", entry)\n",
    "    \n",
    "    _date = re.search('\\d{4}-\\d{2}-\\d{2}', entry)\n",
    "    if _date:\n",
    "        rocket_date.append(\n",
    "            {\n",
    "            f'{sat_name}': _date.group()\n",
    "        })\n",
    "        # return match.group()\n",
    "    else:\n",
    "        print(f'Couldnt find a launching DATE for {sat_name} with entry {entry}')\n",
    "#     print(x.group(1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'Satellite name: {sat_name}')\n",
    "    if(x):\n",
    "#         print(f'Found l')\n",
    "        rocket_list.append(\n",
    "            {\n",
    "            f'{sat_name}': x.group(1)\n",
    "        })\n",
    "    else:\n",
    "        print(f'Couldnt find a launching rocket for {sat_name} with entry {entry}')\n",
    "        \n",
    "    print(f'Processed {index + 1} Satellites out of {len(cont_sats_df.index)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f30a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rocket_list\n",
    "\n",
    "\n",
    "list_of_lists = [list(d.items())[0] for d in rocket_list]\n",
    "# new_list\n",
    "row_names = ['Sat_name', 'L_Rocket']\n",
    "\n",
    "list_of_lists_date = [list(d.items())[0] for d in rocket_date]\n",
    "\n",
    "rockets_date_df = pd.DataFrame(list_of_lists_date, columns=['Sat_name', 'L_Date'])\n",
    "\n",
    "rockets_df = pd.DataFrame(list_of_lists,columns = row_names)\n",
    "\n",
    "\n",
    "## Merge both dataframes, creating the Satellite Launches CSV\n",
    "rockets_df = pd.merge(left=rockets_df, right=rockets_date_df, left_on='Sat_name', right_on='Sat_name')\n",
    "\n",
    "rockets_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_Satellites_Rockets.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22572d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The original sattelite CSV; missing launches\n",
    "sat_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites.csv')\n",
    "# rockets_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites_Rockets.csv')\n",
    "\n",
    "# The Sattelite Launches CSV; missing rest of satellite data\n",
    "rockets_df = pd.read_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_Satellites_Rockets.csv')\n",
    "\n",
    "rockets_df = rockets_df[['Sat_name','L_Date']]\n",
    "\n",
    "\n",
    "merged_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites_Rockets.csv')\n",
    "\n",
    "# Merge both\n",
    "final_df = pd.merge(left=merged_df, right=rockets_df, left_on='Name', right_on='Sat_name', how='left')\n",
    "\n",
    "final_df.shape\n",
    "# merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant and irrelevant columns\n",
    "final_df.drop(columns=['Sat_name', 'Frequency','Launch_Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48febb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate missing date values\n",
    "final_df['L_Date'].fillna('1900-01-01',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "final_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_final_satellites.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a1943",
   "metadata": {},
   "source": [
    "## Scrape Channels and Providers\n",
    "For each satellite, we:\n",
    "1. Scrape the Channels/ Providers records, treating them as one.\n",
    "    a. Identify the Providers and merge with the original dataframe\n",
    "    b. Save that\n",
    "2. Assign Providers to Channels\n",
    "3. Clean the Dataframes\n",
    "    a. Split the System/SR/FEC columns\n",
    "    b. Extract the languages\n",
    "    c. Extract the Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4be60a",
   "metadata": {},
   "source": [
    "We begin by defining *helper functions* & *attributes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d34799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves data tables from a given url\n",
    "def get_table(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html')\n",
    "\n",
    "    table = soup.find_all('table',{'border':\"\", 'cellpadding':\"0\", 'cellspacing':\"0\", 'width':\"720\"} )\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "3d1f6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'https://www.lyngsat.com/'\n",
    "# Contains the names of our Satellites\n",
    "sats_rockets = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites_Rockets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa055949",
   "metadata": {},
   "source": [
    "Get Satellite Safe names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "d46e7537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\('\n",
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_2736\\410144265.py:9: SyntaxWarning: invalid escape sequence '\\('\n",
      "  sat_url_names = [re.split('-\\(',link)[0] for link in raw_sat_names]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare satellite names for incorporation into URLs\n",
    "\n",
    "raw_sat_names = [link.replace(\" \", \"-\") for link in sats_rockets['Name']]\n",
    "raw_sat_names = [link.replace(\"'\", \"\" ) for link in raw_sat_names]\n",
    "raw_sat_names = [link.replace(\"ü\", \"u\" ) for link in raw_sat_names]\n",
    "raw_sat_names = [link.replace(\"/\", \"-\" ) for link in raw_sat_names]\n",
    "raw_sat_names = [link.replace(\"Ä\", \"A\" ) for link in raw_sat_names]\n",
    "\n",
    "sat_url_names = [re.split('-\\(',link)[0] for link in raw_sat_names]\n",
    "# baseurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76135e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_url_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8393d67",
   "metadata": {},
   "source": [
    "We Define a dictionary to map between the original satellite name and its safe name\n",
    "\n",
    "The safe name allows us to store the sattelite name safely on Windows in the filename\n",
    "\n",
    "*This is also useful later when mapping back from the safe name to the original sat. name before import data to mysql*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7615522",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_raw_sat_urls = merged_df['Name']\n",
    "\n",
    "sat_names_dict = dict(zip(sat_url_names, very_raw_sat_urls))\n",
    "sat_names_dict\n",
    "\n",
    "def get_normal_sat_name(safe_sat_name):\n",
    "    return sat_names_dict.get(safe_sat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1e662",
   "metadata": {},
   "source": [
    "### Scrape Channels & Providers\n",
    "\n",
    "Since our focus is on Network Providers & Channels (TV, Radio), we exclude packages from our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22355e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALT\n",
    "\n",
    "## SCRAPE CHANNELS + PROVIDERS\n",
    "# index = 0\n",
    "# sat_name = 'NSS-9'\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "    \n",
    "    url = f'{baseurl}{sat_name}.html'\n",
    "    print(f'{index + 1} Processing satellite {sat_name} with url {url}')\n",
    "\n",
    "    # Scrape endpoint, retrieving all data tables \n",
    "    sat_table = get_table(url)\n",
    "\n",
    "    # Retrieve all tr's from the returned tables\n",
    "    # Returns a list of lists (/table)\n",
    "    column_data = extract_rows_from_tables(sat_table)\n",
    "\n",
    "    for i, table in enumerate(column_data):\n",
    "    #     print(\"table#\",i)\n",
    "        if(len(table) > 2):\n",
    "            _temp = column_data[i]\n",
    "            _temp = _temp[2:len(_temp)-1]\n",
    "        #     print(_temp)\n",
    "            column_data[i] = _temp\n",
    "\n",
    "    # Flattens the list. Now, we have a list of tr tags\n",
    "    column_data = flatten_comprehension(column_data)\n",
    "\n",
    "    # Clean and extract the data values from the tags\n",
    "    channels_extracted = extract_raw_data_alt(column_data)\n",
    "\n",
    "\n",
    "\n",
    "    networks_extracted = extract_providers_link(column_data, pattern='.*providers')\n",
    "\n",
    "\n",
    "\n",
    "    # Define the columns for our Main Dataframe\n",
    "    row_names = ['Freq/beam','SR/FEC', 'SID', 'Provider/Channel','undef','Compression','VPID','Audio', 'Encryption', 'Src_Updated', 'system_alt', 'sr_alt', 'fec_alt', 'freq_alt', 'beam_alt', 'eirp_alt','encryption_alt']\n",
    "\n",
    "    # Define the columns for our Networks Dataframe\n",
    "    netw_row_names = ['Freq/beam','SR/FEC', 'SID', 'Provider/Channel','undef','Compression','VPID','Audio', 'Encryption', 'Src_Updated', 'Provider_URL']\n",
    "\n",
    "    # Construct the Network Dataframe\n",
    "    netw_df = pd.DataFrame(networks_extracted, columns = netw_row_names)\n",
    "\n",
    "    # Add Frequency/Beam to our channels (as inherited from the preceeding element)\n",
    "    channels_extracted = preprocess_extracted_sats(channels_extracted, row_names)\n",
    "\n",
    "    # Add System/SR/FEC to our channels (as inherited from the preceeding element)\n",
    "    channels_extracted = preprocess_extracted_sats(channels_extracted, row_names, 1)\n",
    "\n",
    "    # Construct the Main Dataframe\n",
    "    sat_df = pd.DataFrame(channels_extracted, columns = row_names)\n",
    " \n",
    "    sat_df[['system_alt', 'sr_alt', 'fec_alt']] = sat_df[['system_alt', 'sr_alt', 'fec_alt']].fillna(method='ffill')    \n",
    "    # merged_sat_df['Frequency'] = merged_sat_df['Frequency'].fillna(method='ffill')\n",
    "\n",
    "\n",
    "    # Refine the Networks Dataframe to remove clutter/ redundant attributes\n",
    "    netw_df = netw_df[['Provider/Channel', 'Provider_URL']]\n",
    "\n",
    "    # Merge the Main and Network Dataframes\n",
    "    merged_df = pd.merge(left=sat_df, right=netw_df, how='outer',left_on='Provider/Channel', right_on='Provider/Channel')\n",
    "\n",
    "    # Adjust the filename to be safe (for saving the file on Windows)\n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "\n",
    "    try:\n",
    "        chans_links = extract_providers_link(column_data,pattern='.*tvchannels|.*radiochannels')\n",
    "        # print(chans_links)\n",
    "        chan_links_df = pd.DataFrame(data=chans_links)\n",
    "        chan_links_df = chan_links_df.iloc[:, [1,8]]\n",
    "        chan_links_df.rename(columns ={1: \"Channel\", 8: \"Channel_URL\"}, inplace=True)\n",
    "        merged_df = pd.merge(left=merged_df, right=chan_links_df, left_on='Provider/Channel', right_on='Channel',how='outer')\n",
    "    except:\n",
    "        print(f'Problems with satellite: {safe_sat_name}')\n",
    "    \n",
    "    try:\n",
    "        pkgs_extracted = extract_providers_link(column_data, pattern='.*packages')\n",
    "        if(len(pkgs_extracted) > 0):\n",
    "            pkgs_extracted_df = pd.DataFrame(data=pkgs_extracted)\n",
    "            pkgs_extracted_df= pkgs_extracted_df.iloc[:, [3,21]]\n",
    "            pkgs_extracted_df.rename(columns ={3: \"Pkg_Name\", 21: \"Pkg_URL\"}, inplace=True)\n",
    "            merged_df = pd.merge(left=merged_df, right=pkgs_extracted_df, left_on='Provider/Channel', right_on='Pkg_Name',how='outer')\n",
    "\n",
    "        # print(chan_links_df.shape)\n",
    "        # merged_df.pd_merge(left=merged_df, right )\n",
    "\n",
    "    except: \n",
    "        print(f'problem with pkgs extraction')\n",
    "    # Save the merged Dataframe\n",
    "    merged_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/{index + 1}_{safe_sat_name}_channels.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5e75b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Assign Providers to Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "#     index = 1\n",
    "#     sat_name = 'Intelsat-18'\n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "    my_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/{index + 1}_{safe_sat_name}_channels.csv')\n",
    "\n",
    "   \n",
    "    print(f'{index + 1} Processing sat: {safe_sat_name}')\n",
    "\n",
    "    my_df['is_Provider'] = np.where(my_df['Provider_URL'].astype(str).str.contains('http', regex=True, na=False), True, False)\n",
    "\n",
    "    providers_df = my_df[my_df['is_Provider'] == True]\n",
    "    channels_df = my_df[my_df['is_Provider'] == False]\n",
    "\n",
    "    my_df['Provider'] = None\n",
    "\n",
    "    for i in range(len(my_df.index)):\n",
    "        isProvider = my_df.loc[i, 'is_Provider']\n",
    "    \n",
    "        if isProvider == True:\n",
    "            my_df.loc[i, 'Provider'] = my_df.loc[i, 'Provider/Channel']\n",
    "            continue\n",
    "\n",
    "        if i > 0:\n",
    "            my_df.loc[i, 'Provider'] = my_df.loc[i-1, 'Provider'] \n",
    "    my_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/{index + 1}_{safe_sat_name}_channels.csv',index=False)\n",
    "\n",
    "# my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3f605",
   "metadata": {},
   "source": [
    "## Clean our Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1c51d",
   "metadata": {},
   "source": [
    "##### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "0146f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract string until the last capital character\n",
    "def extract_until_last_capital(s):\n",
    "    match = re.search(r'(.*[A-Z])', s)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return s\n",
    "def find_pattern_and_join(entry):\n",
    "    matches = re.findall(r\"([A-Z][a-z]+)\", entry)\n",
    "    return ' '.join(matches) # Join the matches into a single string\n",
    "def split_beam_eirp(entry, beam_eirp_index = 0):\n",
    "    temp = re.split(r'(?<=\\d)(?=\\D)', entry)\n",
    "    if beam_eirp_index == 1:\n",
    "        # Check if the value is not None\n",
    "        if(len(temp) > 1):\n",
    "            return temp[beam_eirp_index] \n",
    "        else:\n",
    "            'None'\n",
    "    else:\n",
    "        return temp[beam_eirp_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d9e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skipped_sats_url = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeacf91",
   "metadata": {},
   "source": [
    "#### Perform The Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b316bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop irrelevant columns and packages\n",
    "\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "    hor_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/{index + 1}_{safe_sat_name}_channels.csv')\n",
    "\n",
    "    print(f'{index + 1} Processing sat: {safe_sat_name}')\n",
    "\n",
    "    if len(hor_df.index) == 0:\n",
    "        url = f'{baseurl}{sat_name}.html'\n",
    "        skipped_sats_url.append(url)\n",
    "        hor_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/{index+1}_{safe_sat_name}_channels.csv', index=False)\n",
    "        continue\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    hor_df.drop(columns=['undef','Src_Updated', 'Freq/beam', 'Encryption'], inplace = True)  \n",
    "    \n",
    "    # try:\n",
    "    #     hor_df['FEC'] = hor_df['SR/FEC'].str[-3:]\n",
    "    #     hor_df['SR'] = hor_df['SR/FEC'].str[-8:-3].str.extract('(\\d+)')\n",
    "    #     hor_df['SYSTEM'] = hor_df['SR/FEC'].str[0:6]\n",
    "    # except:\n",
    "    #     print(\"PROBLEM YO\")\n",
    "\n",
    "    # Drop the now-old composite column\n",
    "    hor_df.drop('SR/FEC', axis=1, inplace=True)\n",
    "\n",
    "    # Extract Languages\n",
    "    hor_df['Audio'] = hor_df['Audio'].astype(str)\n",
    "\n",
    "    hor_df['Languages'] = hor_df['Audio'].apply(find_pattern_and_join)\n",
    "    \n",
    "    hor_df.drop('Audio', axis=1, inplace=True)\n",
    "    \n",
    "    # if 'A' in df.columns:\n",
    "    if 'Pkg_URL' in hor_df.columns:\n",
    "        # Drop Packages\n",
    "        hor_df.drop(hor_df[hor_df['Pkg_URL'].notna()].index, inplace=True)\n",
    "\n",
    "    \n",
    "    hor_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/{index+1}_{safe_sat_name}_channels.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the URLs that were skipped from cleaning due to lack of data\n",
    "skipped_sats_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64946907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the current output for a cleaned dataframe\n",
    "index = 1\n",
    "safe_sat_name = 'Intelsat-18'\n",
    "\n",
    "my_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v5/assigned/cleaned/{index+1}_{safe_sat_name}_channels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30210e0b",
   "metadata": {},
   "source": [
    "## Get Country for Provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7a079",
   "metadata": {},
   "source": [
    "Define Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Retrieves data tables from a given url\n",
    "def get_country(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html')\n",
    "\n",
    "    table = soup.find_all('table',{'width':\"700\"} )\n",
    "\n",
    "    return table\n",
    "def extract_country(url):\n",
    "    my_pattern = \"/\\D\\D/\"\n",
    "    url = str(url)\n",
    "    # print('url => ', url)\n",
    "    matches = re.search(my_pattern, url)\n",
    "    if matches:\n",
    "        country = matches.group() # Assuming the country is the first captured group\n",
    "        country = country[1:-1]\n",
    "        return country\n",
    "    else:\n",
    "        return None # or any default value you prefer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68629194",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns = ['Sattelite', 'Provider', 'Country'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7224c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Extract Channel Country\n",
    "x_df = None\n",
    "\n",
    "netw_flag = True\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "    \n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "    new_col_name = 'Provider_Country'\n",
    "    ref_col = 'Provider_URL'\n",
    "    dir_name = 'prov_country'\n",
    "    if netw_flag == False:\n",
    "        new_col_name = 'Channel_Country'\n",
    "        ref_col = 'Channel_URL'\n",
    "        dir_name = 'chan_country'\n",
    "        \n",
    "        # May cause problems; remember to change the chan_country dir if cleaning for providers by setting netw_flag\n",
    "    hor_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/chan_country/{index + 1}_{safe_sat_name}_channels.csv')\n",
    "    \n",
    "    # hor_df['Channel_Country'] = None\n",
    "    print(f'{index + 1} Processing sat: {safe_sat_name}')\n",
    "\n",
    "    if len(hor_df.index) == 0:\n",
    "        # url = f'{baseurl}{sat_name}.html'\n",
    "        # skipped_sats_url.append(url)\n",
    "        print(f'Skipping sattelite: {safe_sat_name}')\n",
    "        hor_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/{dir_name}/{index+1}_{safe_sat_name}_channels.csv', index=False)\n",
    "        continue    \n",
    "    \n",
    "    try:\n",
    "        hor_df[new_col_name] = hor_df[ref_col].apply(extract_country)\n",
    "        hor_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/{dir_name}/{index +1}_{safe_sat_name}_channels.csv', index=False)\n",
    "    except:\n",
    "        print('problem yo')\n",
    "        hor_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/{dir_name}/{index +1}_{safe_sat_name}_channels.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec82d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = pd.DataFrame(columns=['Satellite', 'Provider', 'Country'])\n",
    "\n",
    "prov_urls = my_df['Provider_URL'].dropna().unique()\n",
    "\n",
    "# prov_urls = ['https://www.lyngsat.com/tvchannels/us/AFN-Prime-Atlantic.html']\n",
    "for url in prov_urls:\n",
    "    table = get_country(url)\n",
    "    column_data = extract_rows_from_tables(table)\n",
    "\n",
    "    # Flattens the list. Now, we have a list of tr tags\n",
    "    column_data = flatten_comprehension(column_data)\n",
    "    col = extract_raw_data(column_data)\n",
    "    if(len(col) > 1):\n",
    "        provider = my_df[my_df['Provider_URL'] == url].iloc[0]['Provider/Channel']\n",
    "        country = col[0][0].split('\\n')[-1]\n",
    "        new_df.loc[len(new_df)] = [safe_sat_name, provider ,country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/prov_country/1_NSS-9_channels.csv' )\n",
    "\n",
    "# new_df['Sat_Name'] = 'ali'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d59575",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels = pd.DataFrame(columns=new_df.columns)\n",
    "dfs_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd6d7e",
   "metadata": {},
   "source": [
    "## Concatenate Satellites dataframes\n",
    "Here, we concatenate channel instances alongside network providers, which will be the source of construction of our entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde08323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_channels\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "    print(sat_name)\n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "    curr_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v6/assigned/cleaned/prov_country/{index + 1}_{sat_name}_channels.csv')\n",
    "    curr_df['Sattelite_Name'] = safe_sat_name\n",
    "    # all_channels = pd.concat([all_channels, curr_df])\n",
    "    dfs_list.append(curr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc126178",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels = pd.concat(dfs_list, ignore_index=True)\n",
    "# Remove duplicates\n",
    "all_channels.drop_duplicates(inplace=True)\n",
    "all_channels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448670e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_channels.drop(columns=['Src_Updated', 'Encryption', 'Audio', 'undef','SR/FEC', 'Freq/beam'], inplace=True)\n",
    "\n",
    "all_channels.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/all_channels_raw_v6.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b99604",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/all_channels_raw_v6.csv')\n",
    "\n",
    "all_channels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entries that have '(feeds)' as their Provider/Channel name as they are not channels or providers \n",
    "feeds = all_channels[all_channels['Provider/Channel'].astype(str).str.contains('^\\(f.*\\)|^\\[')]\n",
    "\n",
    "indices = feeds.index.values.tolist()  \n",
    "\n",
    "all_channels.drop(axis = 0, index = indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d19e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(temp_all.shape)\n",
    "print(all_channels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same with channels/ provs that start with '@'\n",
    "indices = all_channels[all_channels['Provider/Channel'].astype(str).str.contains('^@')].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7937b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels.drop(axis=0, index=indices, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed10856",
   "metadata": {},
   "source": [
    "Get Channel Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03292fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel_instances_df = all_channels[~all_channels['Channel_URL'].isna()]\n",
    "\n",
    "channel_instances_df = all_channels[all_channels['is_Provider'] == False]\n",
    "\n",
    "# Retrieve records with non-empty channel names\n",
    "channel_instances_df = channel_instances_df[channel_instances_df['Provider/Channel'].notna()]\n",
    "\n",
    "channel_instances_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef800a95",
   "metadata": {},
   "source": [
    "Obtain Unique Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73266758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unique_channels = channel_instances_df.drop_duplicates(subset=['Provider/Channel'])\n",
    "# unique_channels = unique_channels[unique_channels['Provider/Channel'].notna()]\n",
    "\n",
    "unique_channels.shape\n",
    "# df2 = group.apply(lambda x: x['Channel_Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_channels_df = unique_channels[['Provider/Channel','Channel_Country']]\n",
    "entity_channels_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/v6_channels.csv',index=False)\n",
    "entity_channels_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5ffe3",
   "metadata": {},
   "source": [
    "Construct Network Providers Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks_df = all_channels[all_channels['is_Provider'] == True]\n",
    "networks_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_networks = networks_df.drop_duplicates(subset=['Provider'])\n",
    "\n",
    "unique_networks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_networks_df = unique_networks[['Provider', 'Provider_Country']]\n",
    "\n",
    "entity_networks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_networks_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/v6_networks.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f4e9d",
   "metadata": {},
   "source": [
    "Construct Channel Instance Languages Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel_instances_df = channel_instances_df[channel_instances_df['Provider/Channel'].notna()]\n",
    "channel_instances_language = channel_instances_df[['Provider/Channel','Sattelite_Name','freq_alt', 'Languages']]\n",
    "\n",
    "# Split by each space\n",
    "##  The result is a DataFrame where each value in the 'Languages' column is a list of words.\n",
    "channel_instances_language['Languages'] = channel_instances_language['Languages'].str.split(' ')\n",
    "\n",
    "# transform the df by expanding the 'Languages' column\n",
    "## Result: one language per row (atomicity)\n",
    "channel_instances_language = channel_instances_language.explode('Languages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_language.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/v6_channels_instance_language.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112903a",
   "metadata": {},
   "source": [
    "Construct the Channel Instance Encryption entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_encryption = channel_instances_df[['Provider/Channel','Sattelite_Name','freq_alt', 'encryption_alt']]\n",
    "# channel_instances_encryption['encryption_alt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# channel_instances_language = channel_instances_df[['Provider/Channel','Sattelite_Name','freq_alt', 'Languages']]\n",
    "channel_instances_encryption['encryption_alt'] = channel_instances_encryption['encryption_alt'].str.split(',')\n",
    "\n",
    "channel_instances_encryption = channel_instances_encryption.explode('encryption_alt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafb43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel_instances_encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562eb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_encryption.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/v6_channels_instance_encryption.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/channel_instances.csv')\n",
    "# print(channel_instances_df.keys())\n",
    "# channel_instances_df.shape\n",
    "\n",
    "# channel_instances_df[['Sattelite_Name','Provider/Channel','FEC']][101:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_df = channel_instances_df[channel_instances_df['is_Provider'] == False]\n",
    "\n",
    "channel_instances_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435dd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# channel_instances_df[channel_instances_df['Sattelite_Name'] == 'Intelsat-18'][34:100]\n",
    "\n",
    "channel_instances_df.drop_duplicates(subset=['Provider/Channel','Sattelite_Name', 'freq_alt'], inplace=True)\n",
    "# channel_instances_df['Channel'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/v6_channel_instances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b1287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates_df = channel_instances_df[channel_instances_df.duplicated(subset=['Channel','Sattelite_Name', 'freq_alt'], keep=False)]\n",
    "\n",
    "# duplicates_df.shape\n",
    "\n",
    "channel_instances_df['fec_alt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in duplicates_df.keys():\n",
    "    # val_cnt = (duplicates_df[col].isna())\n",
    "    print(f'{col}: {duplicates_df[col].isna().sum()}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abc83e",
   "metadata": {},
   "source": [
    "## Adjusting the Scraping Methodology\n",
    "\n",
    "I found out that my beam, eirp, FEC,and SR attributes were wrong for some records. Hence, I went over it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4fcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_FEC(sus_txt):\n",
    "    match = re.search('^\\d/\\d', sus_txt)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_FEC('2/3'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can see the difference between this version and the one in the very beginning of the notebook\n",
    "def extract_raw_data_alt(column_data,recurse_thru_a = False, find_tags = 'td', extract_provider_links = False, pattern =''):\n",
    "    raw_data = []\n",
    "    for row in column_data:\n",
    "        row_data = row.find_all(find_tags)\n",
    "        columns = row_data\n",
    "        individual_row_data = []\n",
    "        \n",
    "        \n",
    "        if(recurse_thru_a == True):\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "            link_to_page = extracted_links[0] if extracted_links else None\n",
    "            individual_row_data.append(link_to_page)\n",
    "\n",
    "        individual_row_data +=[data.text.strip() for data in row_data]\n",
    "        \n",
    "        # for data in row_data:\n",
    "        font = columns[1].find_all('font')[0]\n",
    "        # font = font.replace('<br/>', ',')\n",
    "        br = font.find_all('br')\n",
    "        \n",
    "        system = None\n",
    "        SR = None\n",
    "        FEC = None\n",
    "        \n",
    "        len_br = len(list(enumerate(br)))\n",
    "        # for i, thing in enumerate(br):\n",
    "        for i, thing in enumerate(br):  \n",
    "            # print(f'{i} -> font: {font}')\n",
    "            # print(f'{i}: {thing}')    \n",
    "            if i == 0:\n",
    "                system = thing.previous_sibling.get_text(strip = True)\n",
    "            if len_br == 3:\n",
    "                if i == 1:\n",
    "                    SR = thing.next_sibling.get_text(strip = True) if thing.next_sibling else None\n",
    "                elif i == 2:\n",
    "                    if thing.next_sibling:\n",
    "                        fec_flag = is_FEC(thing.next_sibling.get_text(strip = True))\n",
    "                        if fec_flag == True:\n",
    "                            FEC = thing.next_sibling.get_text(strip = True)\n",
    "                        else:\n",
    "                            SR = thing.next_sibling.get_text(strip = True)\n",
    "            elif len_br == 2:\n",
    "                if thing.previous_sibling:\n",
    "                    # print(f'prev: {thing.previous_sibling}')\n",
    "                    SR = thing.previous_sibling.get_text(strip = True)\n",
    "                if thing.next_sibling:\n",
    "                    if(is_FEC(thing.next_sibling.get_text(strip = True))):\n",
    "                        FEC = thing.next_sibling.get_text(strip = True)\n",
    "                    else:\n",
    "                        SR = thing.next_sibling.get_text(strip = True)\n",
    "                    \n",
    "        individual_row_data.append(system)\n",
    "        individual_row_data.append(SR)\n",
    "        individual_row_data.append(FEC)\n",
    "                \n",
    "        # print(f'system: {system}, SR: {SR}, FEC: {FEC}')\n",
    "                \n",
    "            # print(f'{i}: {thing.previous_sibling.get_text(strip=True)}')\n",
    "        \n",
    "            # if i == len(br):\n",
    "            #     print('last element')\n",
    "\n",
    "        \n",
    "        # print(f'joined_text: {br}')\n",
    "           \n",
    "        # print(len(columns))\n",
    "        if len(columns) == 10:\n",
    "            font = columns[0].find_all('font')[0]\n",
    "            # print('font: ', font)\n",
    "            \n",
    "            br = font.find_all('br')\n",
    "            # print('br: ', br)\n",
    "            freq = None\n",
    "            beam = None\n",
    "            eirp = None\n",
    "            for i, thing in enumerate(br):\n",
    "                if i == 0:\n",
    "                    freq = thing.previous_sibling.get_text(strip=True)\n",
    "                elif i == 1:\n",
    "                    beam = thing.previous_sibling.get_text(strip=True)\n",
    "                else:\n",
    "                    eirp = thing.next_sibling.get_text(strip=True) if thing.next_sibling else None\n",
    "            # print(f'freq: {freq}, beam: {beam}, eirp: {eirp}')\n",
    "        individual_row_data.append(freq)\n",
    "        individual_row_data.append(beam)\n",
    "        individual_row_data.append(eirp)\n",
    "        \n",
    "        if len(columns) == 10:\n",
    "            fonts = columns[8].find_all('font')\n",
    "            # print('fonts for network: ', fonts)\n",
    "            fonts = [bs(str(data).replace('<br/>',',')).text for data in fonts]\n",
    "            first_font = fonts[0] if fonts else None\n",
    "            individual_row_data.append(first_font)\n",
    "        else:\n",
    "            fonts = columns[6].find_all('font')\n",
    "            fonts = [bs(str(data).replace('<br/>',',')).text for data in fonts]\n",
    "            first_font = fonts[0] if fonts else None\n",
    "            individual_row_data.append(first_font)\n",
    "            # individual_row_data += fonts.pop()\n",
    "        # print('fonts: ', fonts)\n",
    "                        \n",
    "        # individual_row_data +=[data.text.strip() for data in row_data]\n",
    "        \n",
    "        # individual_row_data +=[BeautifulSoup(str(data).replace('<br/>',',')).text for data in row_data]\n",
    "        \n",
    "        if extract_provider_links == True:\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "            for element in extracted_links:\n",
    "                matches = re.findall(pattern, element)\n",
    "                if(matches):\n",
    "                    individual_row_data.append(element)\n",
    "\n",
    "        raw_data.append(individual_row_data)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "channels_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/v6_channels.csv')\n",
    "networks_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/entities/v6_networks.csv')\n",
    "\n",
    "channels_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7bff1",
   "metadata": {},
   "source": [
    "#### Sidenote\n",
    " we apply the match_first_part fn over and over to remove badly-named channels such as 'CHAN NAME (dir 2042- upd 492 -kd42 493299 0 slkd)'\n",
    "\n",
    " * We will use this method below multiple times on many of our entities to ensure consistency *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channels_df.drop_duplicates(subset=['Provider/Channel'], inplace=True)\n",
    "\n",
    "channels_df.loc[:, 'Provider/Channel'] = channels_df['Provider/Channel'].apply(match_first_part)\n",
    "\n",
    "# print(match_first_part(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e52f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels_df['Channel_Country'].isna().value_counts()\n",
    "channels_df['Channel_Country'].fillna('NA', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced5a72",
   "metadata": {},
   "source": [
    "It got a little nasty here because of special characters so I performed a big portion of the cleaning for networks in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b381cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# networks_df['Provider'].replace('Ã', 'A')\n",
    "# networks_df['Provider'].replace('Ü', 'U')\n",
    "\n",
    "replacement_dict = {'Ã': 'A', 'Ü': 'U', 'ó':'o','é':'e'}\n",
    "\n",
    "networks_df = pd.read_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_networks_cleaned.csv')\n",
    "\n",
    "# Apply the replacements to the 'Provider' column\n",
    "networks_df['Provider'] = networks_df['Provider'].replace(replacement_dict, regex=True)\n",
    "\n",
    "networks_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9a214",
   "metadata": {},
   "source": [
    "#### Cleaning Channel Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db18d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_instances_df = pd.read_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fb2c27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider/Channel</th>\n",
       "      <th>Sattelite_Name</th>\n",
       "      <th>freq_alt</th>\n",
       "      <th>beam_alt</th>\n",
       "      <th>sr_alt</th>\n",
       "      <th>fec_alt</th>\n",
       "      <th>Compression</th>\n",
       "      <th>eirp_alt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFN Prime Pacific</td>\n",
       "      <td>NSS-9</td>\n",
       "      <td>4055 L</td>\n",
       "      <td>tp GLL7</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>1/2</td>\n",
       "      <td>MPEG-4/SD</td>\n",
       "      <td>34-35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFN News</td>\n",
       "      <td>NSS-9</td>\n",
       "      <td>4055 L</td>\n",
       "      <td>tp GLL7</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>1/2</td>\n",
       "      <td>MPEG-4/SD</td>\n",
       "      <td>34-35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFN Sports</td>\n",
       "      <td>NSS-9</td>\n",
       "      <td>4055 L</td>\n",
       "      <td>tp GLL7</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>1/2</td>\n",
       "      <td>MPEG-4/SD</td>\n",
       "      <td>34-35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Provider/Channel Sattelite_Name freq_alt beam_alt   sr_alt fec_alt  \\\n",
       "0  AFN Prime Pacific          NSS-9   4055 L  tp GLL7  11000.0     1/2   \n",
       "1           AFN News          NSS-9   4055 L  tp GLL7  11000.0     1/2   \n",
       "2         AFN Sports          NSS-9   4055 L  tp GLL7  11000.0     1/2   \n",
       "\n",
       "  Compression eirp_alt  \n",
       "0   MPEG-4/SD    34-35  \n",
       "1   MPEG-4/SD    34-35  \n",
       "2   MPEG-4/SD    34-35  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_instances_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9936f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns\n",
    "instances_df = channel_instances_df.drop(columns=['VPID','Unnamed: 0', 'encryption_alt', 'Channel','Languages','Provider_URL', 'Channel', 'Channel_URL',\n",
    "       'is_Provider','Channel_Country','Provider_Country','Freq/beam', 'SR/FEC', 'undef', 'Audio', 'Encryption', 'Src_Updated','Pkg_Name', 'Pkg_URL', ])\n",
    "\n",
    "instances_df.loc[:, 'Provider/Channel'] = instances_df['Provider/Channel'].apply(match_first_part)\n",
    "\n",
    "# instances_df.head(0)\n",
    "display_max_length(instances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95654f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_df.head(1)\n",
    "\n",
    "# Retrieve the original Sat. name before importing to mysql\n",
    "instances_df['Sattelite_Name'] = instances_df['Sattelite_Name'].apply(get_normal_sat_name)\n",
    "# instances_df.drop_duplicates(subset=['Provider/Channel', 'Sattelite_Name', 'freq_alt'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_df.shape\n",
    "\n",
    "instances_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_cleaned_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3318b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_df.loc[:, 'Provider/Channel'] = instances_df['Provider/Channel'].apply(match_first_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000dd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_lng_df = pd.read_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_channels_instance_language.csv')\n",
    "\n",
    "instance_lng_df.head(0)\n",
    "instance_lng_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682f1f2",
   "metadata": {},
   "source": [
    "Do the same for Languages and Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9308c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_lng_df.loc[:, 'Provider/Channel'] = instance_lng_df['Provider/Channel'].apply(match_first_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_lng_df.drop_duplicates(subset=['Provider/Channel', 'Sattelite_Name', 'freq_alt', 'Languages'],inplace=True)\n",
    "\n",
    "instance_lng_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_lng_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_lng_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_enc_df = pd.read_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_channels_instance_encryption.csv')\n",
    "instance_enc_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12227acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_enc_df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b683485",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_enc_df.loc[:, 'Provider/Channel'] = instance_enc_df['Provider/Channel'].apply(match_first_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1633e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_enc_df.drop_duplicates(subset=['Provider/Channel', 'Sattelite_Name', 'freq_alt', 'encryption_alt'],inplace=True)\n",
    "\n",
    "instance_enc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb63d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_enc_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_enc_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b38442",
   "metadata": {},
   "source": [
    "When Importing to mysql, I noticed that the FK was violated because satellite names in channel_instances was in the safe format, (e.g. nss-9 not nss 9). Hence, I defined the get_normal_sat_name method and applied it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_lng_df = pd.read_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_lng_cleaned.csv')\n",
    "\n",
    "\n",
    "instance_lng_df['Sattelite_Name'] = instance_lng_df['Sattelite_Name'].apply(get_normal_sat_name)\n",
    "\n",
    "instance_lng_df\n",
    "instance_lng_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_lng_cleaned_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b512e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_enc_df = pd.read_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_enc_cleaned.csv')\n",
    "\n",
    "\n",
    "instance_enc_df['Sattelite_Name'] = instance_enc_df['Sattelite_Name'].apply(get_normal_sat_name)\n",
    "\n",
    "instance_enc_df\n",
    "# instance_enc_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_enc_cleaned_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_enc_df.dropna(subset=['encryption_alt'], inplace=True)\n",
    "\n",
    "instance_enc_df.to_csv('C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/v6_chan_instances_enc_cleaned_1.csv', index = False)\n",
    "# instance_enc_df.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
