{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f04de3",
   "metadata": {},
   "source": [
    "### Perform Necessary Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9bb4e",
   "metadata": {},
   "source": [
    "### Define Helper Functions\n",
    "We will use those throughout our notebook as they make our code resuable, extensible, and readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comprehension(matrix):\n",
    "     return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "def extract_rows_from_tables(tables):\n",
    "    rows = []\n",
    "# Now, iterate over each table in the ResultSet\n",
    "    for table in tables:\n",
    "        # For each table, you can now call find_all on it\n",
    "        rows.append(table.find_all('tr'))\n",
    "        # Do something with rows, e.g., print them\n",
    "        \n",
    "    \n",
    "    return rows\n",
    "\n",
    "def extract_raw_data(column_data,recurse_thru_a = False, find_tags = 'td', extract_provider_links = False, pattern =''):\n",
    "    raw_data = []\n",
    "    for row in column_data:\n",
    "        row_data = row.find_all(find_tags)\n",
    "        individual_row_data = []\n",
    "        \n",
    "        if(recurse_thru_a == True):\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "#             print(\"extracted links: \", extracted_links)\n",
    "            link_to_page = extracted_links[0] if extracted_links else None\n",
    "#             print(f'Link to page: {link_to_page}')\n",
    "            individual_row_data.append(link_to_page)\n",
    "        \n",
    "        individual_row_data +=[data.text.strip() for data in row_data]\n",
    "        \n",
    "        if extract_provider_links == True:\n",
    "            extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "            for element in extracted_links:\n",
    "                matches = re.findall(pattern, element)\n",
    "                if(matches):\n",
    "                    individual_row_data.append(element)\n",
    "#                 else:\n",
    "#                     individual_row_data.append('')\n",
    "        \n",
    "#                 print(f\"'{element}' contains {len(matches)} occurrence(s) of {pattern}\")\n",
    "                \n",
    "#         individual_row_data +=[data.text.strip() for data in row_data]\n",
    "#         print(f'{individual_row_data}')\n",
    "        raw_data.append(individual_row_data)\n",
    "    return raw_data\n",
    "def extract_providers_link(column_data, find_tags = 'td',  pattern = ''):\n",
    "    raw_data = []\n",
    "    for row in column_data:\n",
    "        row_data = row.find_all(find_tags)\n",
    "        individual_row_data = []\n",
    "        \n",
    "       \n",
    "        extracted_links = [data.find('a')['href'] if data.find('a') else data.text.strip() for data in row_data]\n",
    "        \n",
    "        \n",
    "#         print(extracted_links)\n",
    "        for element in extracted_links:\n",
    "            matches = re.findall(pattern, element)\n",
    "            if(matches):\n",
    "                # print(f\"'{element}' contains {len(matches)} occurrence(s) of {pattern}\")\n",
    "                individual_row_data +=[data.text.strip() for data in row_data]\n",
    "                individual_row_data.append(element)\n",
    "                raw_data.append(individual_row_data)\n",
    "\n",
    "                \n",
    "#         individual_row_data +=[data.text.strip() for data in row_data]\n",
    "#         print(f'{individual_row_data}')\n",
    "    return raw_data\n",
    "def append_data_to_df(df, raw_data):\n",
    "    for data in raw_data:\n",
    "#         print(f'data: {data}')\n",
    "        length = len(df)\n",
    "        df.loc[length] = data\n",
    "    return df\n",
    "\n",
    "def preprocess_extracted_sats(cont_sats_extracted, row_names, attr_index = 0):\n",
    "    for index, sat in enumerate(cont_sats_extracted):\n",
    "#         print(\"len(sat) = > \",len(sat))\n",
    "#         print(\"len(row_names) = > \",len(sat))\n",
    "        \n",
    "        if len(sat) <= len(row_names) - 1:\n",
    "#             print(f'found a channel \"{sat}\"')\n",
    "            sat_dir = cont_sats_extracted[index - 1][attr_index]\n",
    "#             print(f'New Attr \"{sat_dir}\"')\n",
    "            sat.insert(attr_index, sat_dir)\n",
    "            cont_sats_extracted[index] = sat\n",
    "#             print(f'New channel \"{cont_sats_extracted[index]}\"')\n",
    "\n",
    "    return cont_sats_extracted\n",
    "\n",
    "def append_region_to_extracted_sats(cont_sats_extracted, continent):\n",
    "    for index, sat in enumerate(cont_sats_extracted):\n",
    "        cont_sats_extracted[index] =  sat + [continent]\n",
    "    return cont_sats_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2e1d9",
   "metadata": {},
   "source": [
    "## Extract Satellites for All Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc79721",
   "metadata": {},
   "source": [
    "### List Continents and concatenate with their url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = ['asia', 'europe', 'atlantic', 'america']\n",
    "baseurl = 'https://www.lyngsat.com/'\n",
    "\n",
    "# Construct the URLs\n",
    "endpoints = []\n",
    "for c in continents:\n",
    "    endpoints.append(f'{baseurl}{c}.html')\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660763f",
   "metadata": {},
   "source": [
    "### Scrape each Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d934f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "satellites = []\n",
    "for index, c in enumerate(continents):\n",
    "    page = requests.get(endpoints[index])\n",
    "    soup = bs(page.text, 'html')\n",
    "    \n",
    "    \n",
    "    satellites.append({f'{c}':soup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5076b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_names = ['Position','Name','Frequency','Launch_Date', 'Region']\n",
    "cont_sats_df = pd.DataFrame(columns = row_names)\n",
    "# \n",
    "for index, cont in enumerate(continents):\n",
    "#     print('continent: ',cont)\n",
    "    cont_sats_raw = satellites[index][cont]\n",
    "    table = cont_sats_raw.find_all('table')[11]\n",
    "    \n",
    "    column_data = table.find_all('tr')\n",
    "    \n",
    "    cont_sats_extracted = extract_raw_data(column_data)\n",
    "    \n",
    "    cont_sats_extracted = preprocess_extracted_sats(cont_sats_extracted, row_names)\n",
    "    cont_sats_extracted = append_region_to_extracted_sats(cont_sats_extracted, cont)    \n",
    "\n",
    "    cont_sats_df = append_data_to_df(cont_sats_df, cont_sats_extracted)\n",
    "    \n",
    "# Switch the first (position) with the second column (name)\n",
    "cont_sats_df = cont_sats_df.iloc[:, [1, 0] + list(range(2, len(cont_sats_df.columns)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b74a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cont_sats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d635d",
   "metadata": {},
   "source": [
    "### Get Launching Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e55944",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_endpoint = baseurl + \"launches/index.html\"\n",
    "\n",
    "page = requests.get(launch_endpoint)\n",
    "soup = bs(page.text, 'html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2dc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find_all('table')[15]\n",
    "column_data = table.find_all('tr')\n",
    "\n",
    "sats_extracted = extract_raw_data(column_data)\n",
    "\n",
    "sats_extracted = [[entry[2], entry[3]] for entry in sats_extracted]\n",
    "row_names = ['Sat_Name', 'Rocket']\n",
    "rockets_df = pd.DataFrame(columns = row_names)\n",
    "\n",
    "rockets_df = append_data_to_df(rockets_df, sats_extracted)\n",
    "# rockets_df\n",
    "\n",
    "merged_df = pd.merge(cont_sats_df, rockets_df, left_on = 'Name', right_on = 'Sat_Name', how='left')\n",
    "# merged_df = merged_df.fillna(None)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_sats_df[['Position_Longitude', 'Position_Direction']] = cont_sats_df['Position'].str.split('\\u00b0', expand=True)\n",
    "cont_sats_df.drop('Position', axis=1, inplace=True)\n",
    "\n",
    "cont_sats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_sats_df.to_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = ['asia', 'europe', 'atlantic', 'america']\n",
    "baseurl = 'https://www.lyngsat.com/tracker/'\n",
    "\n",
    "# Construct the URLs\n",
    "endpoints = []\n",
    "for c in continents:\n",
    "    endpoints.append(f'{baseurl}{c}.html')\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8400915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_rockets = []\n",
    "for index, c in enumerate(continents):\n",
    "    page = requests.get(endpoints[index])\n",
    "    soup = bs(page.text, 'html')\n",
    "    \n",
    "    \n",
    "    sat_rockets.append({f'{c}':soup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666de34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sat_rockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc96590",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_names = ['URL','Position','Name','Frequency', 'Launch Date', 'Region']\n",
    "cont_sats_df = pd.DataFrame(columns = row_names)\n",
    "\n",
    "for index, cont in enumerate(continents):\n",
    "\n",
    "    cont_sats_rockets = sat_rockets[index][cont]\n",
    "# cont_sats_rockets = sat_rockets[2]['atlantic']\n",
    "\n",
    "#     print(cont_sats_rockets)\n",
    "\n",
    "    table = cont_sats_rockets.find_all('table')[11]\n",
    "#     print(table)\n",
    "    \n",
    "    column_data = table.find_all('tr')\n",
    "    \n",
    "#     print(column_data)\n",
    "    \n",
    "    cont_sats_extracted = extract_raw_data(column_data, True)\n",
    "\n",
    "#     cont_sats_extracted = preprocess_extracted_sats(cont_sats_extracted, row_names)\n",
    "    cont_sats_extracted = append_region_to_extracted_sats(cont_sats_extracted, cont)    \n",
    "    \n",
    "    cont_sats_df = append_data_to_df(cont_sats_df, cont_sats_extracted)\n",
    "    \n",
    "    \n",
    "#     print(cont_sats_df)\n",
    "# Switch the first (position) with the second column (name)\n",
    "# cont_sats_df = cont_sats_df.iloc[:, [1, 0] + list(range(2, len(cont_sats_df.columns)))]\n",
    "cont_sats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78960d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_sats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b83561",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_sats_df['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b2bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rocket_list = []\n",
    "for index, cont in enumerate(cont_sats_df['URL']):\n",
    "    url = cont_sats_df['URL'][index]\n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html')\n",
    "#     soup.find_all('font')[13]\n",
    "\n",
    "    entry  = soup.find_all('font')\n",
    "    \n",
    "    entry = ' '.join(map(str,entry))\n",
    "\n",
    "    x = re.search(\"launched with (.+) \\d\\d\\d\\d\", entry)\n",
    "#     print(x.group(1))\n",
    "    \n",
    "    sat_name = cont_sats_df['Name'][index]\n",
    "    \n",
    "    print(f'Satellite name: {sat_name}')\n",
    "    if(x):\n",
    "#         print(f'Found l')\n",
    "        rocket_list.append(\n",
    "            {\n",
    "            f'{sat_name}': x.group(1)\n",
    "        })\n",
    "    else:\n",
    "        print(f'Couldnt find a launching rocket for {sat_name} with entry {entry}')\n",
    "        \n",
    "    print(f'Processed {index + 1} Satellites out of {len(cont_sats_df.index)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f30a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rocket_list\n",
    "\n",
    "\n",
    "list_of_lists = [list(d.items())[0] for d in rocket_list]\n",
    "# new_list\n",
    "row_names = ['Sat_name', 'L_Rocket']\n",
    "\n",
    "rockets_df = pd.DataFrame(list_of_lists,columns = row_names)\n",
    "list_of_lists\n",
    "\n",
    "# new_rockets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22572d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites.csv')\n",
    "# rockets_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites_Rockets.csv')\n",
    "\n",
    "# merged_df = pd.merge(sat_df, rockets_df, left_on = 'Name', right_on = 'Sat_name', how = 'left')\n",
    "# merged_df = merged_df.drop(columns=['Sat_name'])\n",
    "merged_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites_Rockets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a1943",
   "metadata": {},
   "source": [
    "## Scrape Channels and Providers\n",
    "For each satellite, we:\n",
    "1. Scrape the Channels/ Providers records, treating them as one.\n",
    "    a. Identify the Providers and merge with the original dataframe\n",
    "    b. Save that\n",
    "2. Assign Providers to Channels\n",
    "3. Clean the Dataframes\n",
    "    a. Split the System/SR/FEC columns\n",
    "    b. Extract the languages\n",
    "    c. Extract the Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4be60a",
   "metadata": {},
   "source": [
    "We begin by defining *helper functions* & *attributes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d34799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves data tables from a given url\n",
    "def get_table(url):\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.text, 'html')\n",
    "\n",
    "    table = soup.find_all('table',{'border':\"\", 'cellpadding':\"0\", 'cellspacing':\"0\", 'width':\"720\"} )\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'https://www.lyngsat.com/'\n",
    "# Contains the names of our Satellites\n",
    "merged_df = pd.read_csv('E:/AUC/23-24/Spring/Database/Project/CSV Files/Satellites_Rockets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare satellite names for incorporation into URLs\n",
    "\n",
    "raw_sat_names = [link.replace(\" \", \"-\") for link in merged_df['Name']]\n",
    "raw_sat_names = [link.replace(\"'\", \"\" ) for link in raw_sat_names]\n",
    "raw_sat_names = [link.replace(\"ü\", \"u\" ) for link in raw_sat_names]\n",
    "raw_sat_names = [link.replace(\"/\", \"-\" ) for link in raw_sat_names]\n",
    "raw_sat_names = [link.replace(\"Ä\", \"A\" ) for link in raw_sat_names]\n",
    "\n",
    "sat_url_names = [re.split('-\\(',link)[0] for link in raw_sat_names]\n",
    "# baseurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76135e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_url_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1e662",
   "metadata": {},
   "source": [
    "### Scrape Channels & Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0bd92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SCRAPE CHANNELS + PROVIDERS\n",
    "\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "\n",
    "    url = f'{baseurl}{sat_name}.html'\n",
    "    print(f'{index + 1} Processing satellite {sat_name} with url {url}')\n",
    "\n",
    "    # Scrape endpoint, retrieving all data tables \n",
    "    sat_table = get_table(url)\n",
    "\n",
    "    # Retrieve all tr's from the returned tables\n",
    "    # Returns a list of lists (/table)\n",
    "    column_data = extract_rows_from_tables(sat_table)\n",
    "\n",
    "    for i, table in enumerate(column_data):\n",
    "    #     print(\"table#\",i)\n",
    "        if(len(table) > 2):\n",
    "            _temp = column_data[i]\n",
    "            _temp = _temp[2:len(_temp)-1]\n",
    "        #     print(_temp)\n",
    "            column_data[i] = _temp\n",
    "\n",
    "    # Flattens the list. Now, we have a list of tr tags\n",
    "    column_data = flatten_comprehension(column_data)\n",
    "   \n",
    "    # Clean and extract the data values from the tags\n",
    "    channels_extracted = extract_raw_data(column_data)\n",
    "    networks_extracted = extract_providers_link(column_data, pattern='.*providers')\n",
    "    \n",
    "\n",
    "\n",
    "    # Define the columns for our Main Dataframe\n",
    "    row_names = ['Freq/beam','SR/FEC', 'SID', 'Provider/Channel','undef','Compression','VPID','Audio', 'Encryption', 'Src_Updated']\n",
    "    \n",
    "    # Define the columns for our Networks Dataframe\n",
    "    netw_row_names = ['Freq/beam','SR/FEC', 'SID', 'Provider/Channel','undef','Compression','VPID','Audio', 'Encryption', 'Src_Updated', 'Provider_URL']\n",
    "    \n",
    "    # Construct the Network Dataframe\n",
    "    netw_df = pd.DataFrame(networks_extracted, columns = netw_row_names)\n",
    "    \n",
    "    # Add Frequency/Beam to our channels (as inherited from the preceeding element)\n",
    "    channels_extracted = preprocess_extracted_sats(channels_extracted, row_names)\n",
    "\n",
    "    # Add System/SR/FEC to our channels (as inherited from the preceeding element)\n",
    "    channels_extracted = preprocess_extracted_sats(channels_extracted, row_names, 1)\n",
    "\n",
    "    # Construct the Main Dataframe\n",
    "    sat_df = pd.DataFrame(channels_extracted, columns = row_names)    \n",
    "    \n",
    "    # Refine the Networks Dataframe to remove clutter/ redundant attributes\n",
    "    netw_df = netw_df[['Provider/Channel', 'Provider_URL']]\n",
    "\n",
    "    # Merge the Main and Network Dataframes\n",
    "    merged_df = pd.merge(left=sat_df, right=netw_df, how='outer',left_on='Provider/Channel', right_on='Provider/Channel')\n",
    "\n",
    "    # Adjust the filename to be safe (for saving the file on Windows)\n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "\n",
    "    # Save the merged Dataframe\n",
    "    merged_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v2/{index + 1}_{safe_sat_name}_channels.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5e75b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Assign Providers to Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "#     index = 1\n",
    "#     sat_name = 'Intelsat-18'\n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "    my_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v2/{index + 1}_{safe_sat_name}_channels.csv')\n",
    "\n",
    "    # if len(my_df.index) == 0:\n",
    "    #     continue\n",
    "    print(f'{index + 1} Processing sat: {safe_sat_name}')\n",
    "\n",
    "    my_df['is_Provider'] = np.where(my_df['Provider_URL'].astype(str).str.contains('http', regex=True, na=False), True, False)\n",
    "\n",
    "    providers_df = my_df[my_df['is_Provider'] == True]\n",
    "    channels_df = my_df[my_df['is_Provider'] == False]\n",
    "\n",
    "    my_df['Provider'] = None\n",
    "\n",
    "\n",
    "    for i in range(len(my_df.index)):\n",
    "        isProvider = my_df.loc[i, 'is_Provider']\n",
    "    \n",
    "        if isProvider == True:\n",
    "            my_df.loc[i, 'Provider'] = my_df.loc[i, 'Provider/Channel']\n",
    "            continue\n",
    "\n",
    "        if i > 0:\n",
    "            my_df.loc[i, 'Provider'] = my_df.loc[i-1, 'Provider'] \n",
    "    my_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v2/assigned/{index + 1}_{safe_sat_name}_channels.csv',index=False)\n",
    "\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3f605",
   "metadata": {},
   "source": [
    "### Clean our Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1c51d",
   "metadata": {},
   "source": [
    "##### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to extract string until the last capital character\n",
    "def extract_until_last_capital(s):\n",
    "    match = re.search(r'(.*[A-Z])', s)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return s\n",
    "def find_pattern_and_join(entry):\n",
    "    matches = re.findall(r\"([A-Z][a-z]+)\", entry)\n",
    "    return ' '.join(matches) # Join the matches into a single string\n",
    "def split_beam_eirp(entry, beam_eirp_index = 0):\n",
    "    temp = re.split(r'(?<=\\d)(?=\\D)', entry)\n",
    "    if beam_eirp_index == 1:\n",
    "        # Check if the value is not None\n",
    "        if(len(temp) > 1):\n",
    "            return temp[beam_eirp_index] \n",
    "        else:\n",
    "            'None'\n",
    "    else:\n",
    "        return temp[beam_eirp_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1d9e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skipped_sats_url = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeacf91",
   "metadata": {},
   "source": [
    "#### Perform The Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b316bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for index, sat_name in enumerate(sat_url_names):\n",
    "    safe_sat_name = sat_name.replace('/', '-') # Replace '/' with '_'\n",
    "    hor_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v2/assigned/{index + 1}_{safe_sat_name}_channels.csv')\n",
    "\n",
    "    print(f'{index + 1} Processing sat: {safe_sat_name}')\n",
    "\n",
    "    if len(hor_df.index) == 0:\n",
    "        url = f'{baseurl}{sat_name}.html'\n",
    "        skipped_sats_url.append(url)\n",
    "        hor_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v2/assigned/cleaned/{index+1}_{safe_sat_name}_channels.csv', index=False)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    # Drop irrelevant columns\n",
    "    hor_df.drop(columns=['undef','Src_Updated'], inplace = True)  \n",
    "\n",
    "    try:\n",
    "        hor_df['FEC'] = hor_df['SR/FEC'].str[-3:]\n",
    "        hor_df['SR'] = hor_df['SR/FEC'].str[-8:-3].str.extract('(\\d+)')\n",
    "        hor_df['SYSTEM'] = hor_df['SR/FEC'].str[0:6]\n",
    "    except:\n",
    "        print(\"PROBLEM YO\")\n",
    "\n",
    "    # Drop the now-old composite column\n",
    "    hor_df.drop('SR/FEC', axis=1, inplace=True)\n",
    "\n",
    "    # Extract Languages\n",
    "    hor_df['Audio'] = hor_df['Audio'].astype(str)\n",
    "\n",
    "    hor_df['Languages'] = hor_df['Audio'].apply(find_pattern_and_join)\n",
    "    \n",
    "    hor_df.drop('Audio', axis=1, inplace=True)\n",
    "    \n",
    "    # Split the Composite Frequency column    \n",
    "    try:\n",
    "#         extract_until_last_capital\n",
    "        hor_df['Freq'] = hor_df['Freq/beam'].str[0:7].apply(extract_until_last_capital)\n",
    "    except:\n",
    "        print(\"PROBLEMSSSSSS\")\n",
    "    \n",
    "    hor_df.to_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v2/assigned/cleaned/{index+1}_{safe_sat_name}_channels.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the URLs that were skipped from cleaning due to lack of data\n",
    "skipped_sats_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64946907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the current output for a cleaned dataframe\n",
    "index = 1\n",
    "safe_sat_name = 'Intelsat-18'\n",
    "\n",
    "my_df = pd.read_csv(f'E:/AUC/23-24/Spring/Database/Project/CSV Files/providers/v2/assigned/cleaned/{index+1}_{safe_sat_name}_channels.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
